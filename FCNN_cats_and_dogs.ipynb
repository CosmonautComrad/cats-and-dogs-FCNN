{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12288, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./training_set/\"\n",
    "val_data_path = \"./validation_set/\"\n",
    "test_data_path = \"./test_set/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms.\n",
    "\n",
    "### Resize(64):\n",
    "Изображения, на которых будет происходить обучение представлены во многших разрешениях, чтобы повысить производительность обработки, мы масштабируем каждое входящее изображение до разрешения 64 х 64.\n",
    "\n",
    "### ToTensor():\n",
    "Превращаем изображения в тензор.\n",
    "\n",
    "### Normalize:\n",
    "Нормализуем тензор вокруг определённого набора средних и стандартных точек отклонения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "device = 'cpu'\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle = True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle = True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(simplenet.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Training Loss: {training_loss}, Validation Loss: {valid_loss}, Accuracy: {num_correct / num_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-59f5e8768372>:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n",
      "<ipython-input-47-1a561919def2>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.6892675452232361, Validation Loss: 0.6574349241256714, Accuracy: 0.621\n",
      "Epoch: 2, Training Loss: 0.6343324465751647, Validation Loss: 0.660327317237854, Accuracy: 0.613\n",
      "Epoch: 3, Training Loss: 0.6120355310440063, Validation Loss: 0.648521062374115, Accuracy: 0.628\n",
      "Epoch: 4, Training Loss: 0.5848404750823974, Validation Loss: 0.6463684635162353, Accuracy: 0.63\n",
      "Epoch: 5, Training Loss: 0.5607264590263367, Validation Loss: 0.6410220942497253, Accuracy: 0.645\n",
      "Epoch: 6, Training Loss: 0.5411466283798217, Validation Loss: 0.6430005655288696, Accuracy: 0.64\n",
      "Epoch: 7, Training Loss: 0.5218143277168273, Validation Loss: 0.6441792917251586, Accuracy: 0.639\n",
      "Epoch: 8, Training Loss: 0.5129388098716736, Validation Loss: 0.652939893245697, Accuracy: 0.641\n",
      "Epoch: 9, Training Loss: 0.49121302938461303, Validation Loss: 0.653381667137146, Accuracy: 0.642\n",
      "Epoch: 10, Training Loss: 0.48360399770736695, Validation Loss: 0.6530035829544067, Accuracy: 0.638\n",
      "Epoch: 11, Training Loss: 0.46114123129844664, Validation Loss: 0.6526314749717712, Accuracy: 0.634\n",
      "Epoch: 12, Training Loss: 0.45974380350112914, Validation Loss: 0.6576117005348205, Accuracy: 0.627\n",
      "Epoch: 13, Training Loss: 0.4464552817344666, Validation Loss: 0.6569018530845642, Accuracy: 0.634\n",
      "Epoch: 14, Training Loss: 0.44274908566474913, Validation Loss: 0.6411743021011352, Accuracy: 0.654\n",
      "Epoch: 15, Training Loss: 0.4378975296020508, Validation Loss: 0.6553846583366394, Accuracy: 0.638\n",
      "Epoch: 16, Training Loss: 0.4253549811840057, Validation Loss: 0.662739652633667, Accuracy: 0.632\n",
      "Epoch: 17, Training Loss: 0.42283046889305115, Validation Loss: 0.654775873184204, Accuracy: 0.636\n",
      "Epoch: 18, Training Loss: 0.41552841234207155, Validation Loss: 0.6434745097160339, Accuracy: 0.652\n",
      "Epoch: 19, Training Loss: 0.40815388822555543, Validation Loss: 0.6623804464340209, Accuracy: 0.626\n",
      "Epoch: 20, Training Loss: 0.40758034062385556, Validation Loss: 0.6473180918693543, Accuracy: 0.653\n",
      "Epoch: 21, Training Loss: 0.3972357406616211, Validation Loss: 0.6485555830001831, Accuracy: 0.646\n",
      "Epoch: 22, Training Loss: 0.39594344997406006, Validation Loss: 0.661228530406952, Accuracy: 0.635\n",
      "Epoch: 23, Training Loss: 0.4127470018863678, Validation Loss: 0.6565286741256714, Accuracy: 0.645\n",
      "Epoch: 24, Training Loss: 0.3968548378944397, Validation Loss: 0.6620906248092652, Accuracy: 0.642\n",
      "Epoch: 25, Training Loss: 0.38341560125350954, Validation Loss: 0.6595486888885498, Accuracy: 0.641\n",
      "Epoch: 26, Training Loss: 0.38408827686309815, Validation Loss: 0.6566630325317383, Accuracy: 0.64\n",
      "Epoch: 27, Training Loss: 0.3963724000453949, Validation Loss: 0.6441535725593567, Accuracy: 0.659\n",
      "Epoch: 28, Training Loss: 0.38276547789573667, Validation Loss: 0.6493660607337952, Accuracy: 0.653\n",
      "Epoch: 29, Training Loss: 0.3835656940937042, Validation Loss: 0.6634411706924438, Accuracy: 0.642\n",
      "Epoch: 30, Training Loss: 0.3788464574813843, Validation Loss: 0.6558950867652893, Accuracy: 0.647\n",
      "Epoch: 31, Training Loss: 0.3802530951499939, Validation Loss: 0.6637860651016235, Accuracy: 0.636\n",
      "Epoch: 32, Training Loss: 0.37888391423225404, Validation Loss: 0.6541690497398377, Accuracy: 0.649\n",
      "Epoch: 33, Training Loss: 0.37628861784934997, Validation Loss: 0.6503139796257019, Accuracy: 0.651\n",
      "Epoch: 34, Training Loss: 0.37212283301353455, Validation Loss: 0.6660128307342529, Accuracy: 0.633\n",
      "Epoch: 35, Training Loss: 0.36847074890136716, Validation Loss: 0.6549984488487244, Accuracy: 0.647\n",
      "Epoch: 36, Training Loss: 0.36568037056922914, Validation Loss: 0.6537058615684509, Accuracy: 0.649\n",
      "Epoch: 37, Training Loss: 0.36815121245384214, Validation Loss: 0.6555783252716064, Accuracy: 0.648\n",
      "Epoch: 38, Training Loss: 0.35951759791374205, Validation Loss: 0.6642518186569214, Accuracy: 0.635\n",
      "Epoch: 39, Training Loss: 0.36472988104820253, Validation Loss: 0.6653047347068787, Accuracy: 0.632\n",
      "Epoch: 40, Training Loss: 0.3730699305534363, Validation Loss: 0.6737170686721802, Accuracy: 0.625\n",
      "Epoch: 41, Training Loss: 0.3716573176383972, Validation Loss: 0.6671250810623169, Accuracy: 0.637\n",
      "Epoch: 42, Training Loss: 0.3633085560798645, Validation Loss: 0.6715349550247193, Accuracy: 0.629\n",
      "Epoch: 43, Training Loss: 0.3692980673313141, Validation Loss: 0.6663408551216126, Accuracy: 0.636\n",
      "Epoch: 44, Training Loss: 0.36849198627471924, Validation Loss: 0.6639280915260315, Accuracy: 0.634\n",
      "Epoch: 45, Training Loss: 0.3699028344154358, Validation Loss: 0.6800617799758911, Accuracy: 0.622\n",
      "Epoch: 46, Training Loss: 0.3704955892562866, Validation Loss: 0.6599224605560303, Accuracy: 0.643\n",
      "Epoch: 47, Training Loss: 0.3617506010532379, Validation Loss: 0.6722045121192932, Accuracy: 0.63\n",
      "Epoch: 48, Training Loss: 0.3598785712718964, Validation Loss: 0.6605855841636658, Accuracy: 0.644\n",
      "Epoch: 49, Training Loss: 0.36339705443382264, Validation Loss: 0.6757901787757874, Accuracy: 0.628\n",
      "Epoch: 50, Training Loss: 0.37274094796180723, Validation Loss: 0.6770008563995361, Accuracy: 0.626\n",
      "Wall time: 13min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a dog!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-59f5e8768372>:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n",
      "<ipython-input-50-c130a6b8391f>:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prediction = F.softmax(simplenet(img))\n"
     ]
    }
   ],
   "source": [
    "labels = [\"It's a cat!\", \"It's a dog!\"]\n",
    "\n",
    "img = Image.open(\"./test_set/dogs/dog.4503.JPG\") \n",
    "img = img_transforms(img).to(device)\n",
    "\n",
    "\n",
    "prediction = F.softmax(simplenet(img))\n",
    "prediction = prediction.argmax()\n",
    "print(labels[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
